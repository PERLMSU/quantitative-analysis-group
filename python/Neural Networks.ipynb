{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Neural networks are a machine learning technique that function similarily to the neurons in our brain. In this notebook, we'll cover some of the basics of neural networks and walk through an example or two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Function\n",
    "Neural networks have three parts: the inputs, the outputs, and the \"hidden\" layers. The inputs are the data feed into the neural network and the outputs are the predictions of the neural network. The hidden layers are the inner workings of the neural network and are were the magic happens. Well, not really magic, just math.\n",
    "\n",
    "There can be multiple hidden layers but for this example, let's just have a single hidden layer. Suppose we have three inputs $x_1, x_2, x_3$ and are trying to predict $y$. The hidden layer creates a weighted sum of the input before returning an output value. Mathematically, if the $x_i$ input has weight $W_i$, the output from the hidden layer is $y_{hidden}=W_1*x_1 + W_2*x_2 + W_3*x_3$. A hidden layer can have multiple neurons so we'd have to find $y_{hidden}$ for each neuron. \n",
    "\n",
    "Here's where the \"neural\" part comes into play. In the brain, neurons only fire if the input signals are above a certain threshold. To create that threshold, an activation function is used, which  tells the neuron whether to \"fire\" or not. While the activation function could be chosen to be a step function to simulate this binary nature, a sigmoid function, $f(x)=\\frac{1}{1+e^{-x}}$, is often used so that we instead get a probability of the neuron firing (since the sigmoid function varies between 0 and 1). The output of the hidden layer is passed through the activation function which then gives the actual output of that hidden layer neuron. In our single hidden layer example, the hidden layer's output will be the output of the neural network since there are no more layers to pass through.\n",
    "\n",
    "Let's actually do an example now to see how this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "The code is shamelessly based off an example I found at https://iamtrask.github.io/2015/07/12/basic-python-network/. I've changed variable names to make it easier to follow.\n",
    "\n",
    "First, let's import numpy since we'll need to for all the data manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some training data. Let's suppose we have three binary inputs and a single binary output.\n",
    "\n",
    "| Input 1 | Input 2 | Input 3 | Output |   |\n",
    "|---------|---------|---------|--------|---|\n",
    "| 0       | 0       | 1       | 0      |   |\n",
    "| 1       | 1       | 1       | 1      |   |\n",
    "| 1       | 0       | 1       | 1      |   |\n",
    "| 0       | 1       | 1       | 0      |   |\n",
    "\n",
    "Let's call the input array $X$ and the outputs $y$. The \".T\" is so that the output is a column vector rather than a row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are unknown and the goal of the neural network is to find the optimal values for the weights to make accurate predictions. We start by randomly assigning weights to each connection between the inputs and the outputs. Here, we pick the weights to fall in [-1,1] so that the mean is 0. (There's a whole theory behind the weight initialization but I didn't really look into it beyond that having a mean of zero is good). Since we have three inputs and want a single output, we need 3 initial weights. These are stored in a 3x1 array. In general, the array is of size (number inputs)x(number outputs). \n",
    "\n",
    "For replicability, we will set a seed. This just means as long as the seed is the same, we'll always get the same random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "weights=2*np.random.random((3,1))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we need to define our activation function. We'll use the sigmoid one in this example. The $\\tanh$ function and the rectified linear function $ReLu$ are other popular choices so feel free to see how the results differ based on the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the training. Since the inputs are in an array and the weights are in an array, we just use the dot product to multiply everything together. To get the output, we just pass the result through the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=sigmoid(np.dot(X,weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model did with the actual outputs as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we randomly picked the weights, we shouldn't expect much from the model. The strength of the neural network comes from backpropagation, which allows the network to update itself after seeing the predicting. Essentially, a loss function is calculated after each run and the neural network adjusts the weights to minimize the loss function.\n",
    "\n",
    "One way to do this is using gradient descent, which is based on the derivative of the activation function. Since the derivative at a minimum is zero, the sign of the derivative gives insight into how the weights should be adjusted. Here, we will adjust the weights by the Delta rule which means the changes in the weights are proportional to the derivative of the activation function times the error times the value of the input.\n",
    "\n",
    "Let's add the backpropagation stuff now. It's useful to the know the derivative of the sigmoid function, $S(x)$, is $S^\\prime(x)=S(x)*(1-S(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dsigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors=y-output\n",
    "\n",
    "#the changes to the weights\n",
    "deltas=errors*Dsigmoid(output)\n",
    "\n",
    "#now the new weights\n",
    "weights +=np.dot(X.T,deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train again and look at the new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=sigmoid(np.dot(X,weights))\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pretty far off, so let's make a loop repeat this process a few thousand times and see how it looks at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    output=sigmoid(np.dot(X,weights))\n",
    "    error=y-output\n",
    "    delta=error*Dsigmoid(output)\n",
    "    weights +=np.dot(X.T,delta)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These represent the predicted outputs of the training data. Remember, the correct responses are $0, 0, 1, 1$ so the neural network did a pretty good job.\n",
    "\n",
    "Since we have three binary variables, there are 8 possible combinations. Let's test the neural network on the other 4 and see how well it can predict their outputs. \n",
    "\n",
    "| Input 1 | Input 2 | Input 3 | Output |   |\n",
    "|---------|---------|---------|--------|---|\n",
    "| 0       | 0       | 0       | 0      |   |\n",
    "| 1       | 0       | 0       | 1      |   |\n",
    "| 0       | 1       | 0       | 0      |   |\n",
    "| 1       | 1       | 0       | 1      |   |\n",
    "To get the outputs, we note that in the original data, the output was always the same as input 1 so we have applied that pattern in generating the outputs for this new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.array([  [0,0,0],\n",
    "                [1,0,0],\n",
    "                [0,1,0],\n",
    "                [1,1,0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output=sigmoid(np.dot(test,weights))\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, we were able to predict the cases with $1$ as the output successfully but not the cases with $0$ as the output. This is likely due to the how we trained the model. Recall that input 3 was always $1$ while input 1 and input 2 varied. Here, input 3 is fixed as $0$ which is something the neural network hasn't seen before. Nevertheless, the neural network is still correct about 50% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer networks\n",
    "In the first example, the output was just the value in input 1 so the relationship was linear and a single layer was sufficient. Now, let's change the output values a bit so that we have a nonlinear relationship\n",
    "\n",
    "| Input 1 | Input 2 | Input 3 | Output |   |\n",
    "|---------|---------|---------|--------|---|\n",
    "| 0       | 0       | 1       | 0      |   |\n",
    "| 0       | 1       | 1       | 1      |   |\n",
    "| 1       | 0       | 1       | 1      |   |\n",
    "| 1       | 1       | 1       | 0      |   |\n",
    "\n",
    "The result clearly depends on inputs 1 and 2 now. To make a neural network for this problem, we need a multilayer network. For simplicity, let's use two hidden layers this time. Since the first hidden layer maps to the second hidden layer instead of the output now, we can have as many neurons as we want in the first hidden layer. Let's choose 5.\n",
    "\n",
    "The initial code is basically the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "y2 = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "l1_weights=2*np.random.random((3,5))-1\n",
    "l2_weights=2*np.random.random((5,1))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (3,5) forms the matrix that connects the 3 inputs to the 5 neurons in the first hidden layer. The 5 can be changed to any number as long as the 5 in the (5,1) is changed as well since this creates the matrix that connects the 5 neurons in the first hidden layer to the single output\n",
    "\n",
    "We now pass each input through the first hidden layer and then the output of the first hidden layer to the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_output=sigmoid(np.dot(X2,l1_weights))\n",
    "l2_output=sigmoid(np.dot(l1_output,l2_weights))\n",
    "print(l2_output)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the backpropagation, we need to update the weights of both the first and second hidden layers. We start by finding the error with respect to the final layer and update those weights the same way as in the previous example. To find the changes in the weights of the first hidden layer, we need to find out how much the first hidden layer weights contributed to the error in the second hidden layer weights. This is just the dot product of the first hidden layer weights and the deltas for the second hidden layer weights.\n",
    "\n",
    "The changes in the weights are still found by the Delta rule: derivative of activation function evaluated at the output of that hidden layer times the error of the layer times the input of that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_error=y2-l2_output\n",
    "l2_delta=l2_error*Dsigmoid(l2_output)\n",
    "l1_error=np.dot(l2_delta,l2_weights.T)\n",
    "l1_delta=l1_error*Dsigmoid(l1_output)\n",
    "l2_weights += np.dot(l1_output.T,l2_delta)\n",
    "l1_weights += np.dot(X2.T,l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_output=sigmoid(np.dot(X2,l1_weights))\n",
    "l2_output=sigmoid(np.dot(l1_output,l2_weights))\n",
    "print(l2_output)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's create a loop to do this process a few thousand times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(10000):\n",
    "    l1_output=sigmoid(np.dot(X2,l1_weights))\n",
    "    l2_output=sigmoid(np.dot(l1_output,l2_weights))\n",
    "    \n",
    "    l2_error=y2-l2_output\n",
    "    l2_delta=l2_error*Dsigmoid(l2_output)\n",
    "    l1_error=np.dot(l2_delta,l2_weights.T)\n",
    "    l1_delta=l1_error*Dsigmoid(l1_output)\n",
    "    \n",
    "    l2_weights += np.dot(l1_output.T,l2_delta)\n",
    "    l1_weights += np.dot(X2.T,l1_delta)\n",
    "print(l2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the neural network works pretty well.\n",
    "\n",
    "We can keep adding more hidden layers to handle more complex data. This is the basis for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciKit-Learn Example\n",
    "So far, we've just built a simple neural network from scratch on simple data. Let's use some real data now. We'll use the Breast Cancer dataset in SciKit-Learn's library with the goal of predicting whether a tumor is cancerous or not based on a variety of factors.\n",
    "\n",
    "Example based off https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-f68d2b2478e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'head' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=cancer['data']\n",
    "y=cancer['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the other machine learning techniques, we should really train the neural network on some of the data and then test the model on new data. We can use the *train_test_split()* function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default SciKit learn version of neural networks keep updating the weights until the model output is within some tolerance of the actual output. Therefore, preprocessing the data is especially important so that the training converges. *StandardScaler* sets the mean to zero and the variance to 1 so we use that. The same scale should be applied to the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the scale based on the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the scale to the training and testing data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the neural network function from SciKit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of parameters that can be adjusted, but let's just focus on the hidden layers. We can select how many layers we want and how many neurons to be in each. For all the parameters and what they do, go to http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "Since we have 30 inputs, let's pick all the hidden layers to have 30 inputs. Let's use 3 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(30,30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now train the model\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well the model works. We'll predict the test data from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a confusion matrix to look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  4]\n",
      " [ 1 90]]\n"
     ]
    }
   ],
   "source": [
    "#actual are rows, predictions are columns\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We have a few misclassifications so let's try changing the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp4=MLPClassifier(hidden_layer_sizes=(30,30,30,30))\n",
    "mlp_more=MLPClassifier(hidden_layer_sizes=(30,40,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 40, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp4.fit(X_train,y_train)\n",
    "mlp_more.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions4=mlp4.predict(X_test)\n",
    "predictions_more=mlp_more.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  2]\n",
      " [ 1 90]]\n",
      "[[49  3]\n",
      " [ 1 90]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions4))\n",
    "print(confusion_matrix(y_test,predictions_more))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can get slight improvement with more layers or by adding more neurons. Feel free to play around with the hidden layers or the other parameters and see if you can get a perfect classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
